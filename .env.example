# ========================================
# Multi-AI Conversation Manager
# Environment Variables Template
# ========================================
#
# Copy this file to .env.local and add your actual API keys
# Command: cp .env.example .env.local
#
# IMPORTANT: Never commit .env.local to git!
# ========================================

# ----------------------------------------
# OpenAI Configuration
# ----------------------------------------
# Get your API key from: https://platform.openai.com/api-keys
NEXT_PUBLIC_OPENAI_API_KEY=your_openai_api_key_here

# ----------------------------------------
# Anthropic (Claude) Configuration
# ----------------------------------------
# Get your API key from: https://console.anthropic.com/
NEXT_PUBLIC_ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ----------------------------------------
# Google Gemini Configuration
# ----------------------------------------
# Get your API key from: https://makersuite.google.com/app/apikey
NEXT_PUBLIC_GEMINI_API_KEY=your_gemini_api_key_here

# ----------------------------------------
# Ollama Configuration
# ----------------------------------------
# You can use BOTH Ollama Local and Ollama Cloud in the same application!
#
# For LOCAL Ollama installation:
# - Install from: https://ollama.ai
# - Default URL is http://localhost:11434 (hardcoded in the app)
# - No API key needed for local installation
# - Select "Ollama (Local)" provider in the UI
#
# For OLLAMA CLOUD (can run simultaneously with local):
# - Sign up at: https://ollama.ai/cloud (or appropriate cloud service)
# - Get your API key from your account dashboard
# - Add your key below to enable cloud features
# - Select "Ollama Cloud" provider in the UI
#
# IMPORTANT: Using both OpenAI and Ollama Cloud together:
# - Keep your real OpenAI credentials in NEXT_PUBLIC_OPENAI_API_KEY above
# - Add your Ollama Cloud credentials here in NEXT_PUBLIC_OLLAMA_API_KEY
# - The app overrides the API endpoint per provider, so no conflicts occur
# - Each provider is instantiated separately with its own base URL and key
NEXT_PUBLIC_OLLAMA_API_KEY=your_ollama_cloud_api_key_here
NEXT_PUBLIC_OLLAMA_URL=https://api.ollama.ai

# Ollama Model Selection:
# - For local: Check your installed models with: ollama list
# - Common models: llama3.3:latest, llama2, mistral, codellama, deepseek-r1:70b
# - For cloud: Check available models in your provider's documentation
NEXT_PUBLIC_OLLAMA_MODEL=llama3.3:latest

# ----------------------------------------
# Notes:
# ----------------------------------------
# 1. All variables must start with NEXT_PUBLIC_ to be accessible in the browser
# 2. Leave API keys empty for providers you don't want to use
# 3. The app will show an error if you try to use a provider without an API key
# 4. You can configure API keys in the app's Settings panel as well
# 5. For Ollama (Local), you only need the Ollama desktop app installed
